{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbhIsrgUvOje",
        "outputId": "f06386e8-12b0-448a-97db-52a52fcea6cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install -U scikit-image\n",
        "!pip -q install einops\n",
        "!pip -q install lmdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install thefuzz\n",
        "from thefuzz import fuzz\n",
        "from thefuzz import process"
      ],
      "metadata": {
        "id": "Tk1wBfvbrVzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load model and weights\n",
        "!gdown 1VU-qucVHcNbu2qnGkWWNwO0TRhY-7iH2\n",
        "!unzip -q vietocr.zip\n",
        "!unzip -q weights.zip\n",
        "\n",
        "#Import model\n",
        "from vietocr import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LTZrtlcvULH",
        "outputId": "94b3e125-4349-4d28-e92b-cbf7459738c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VU-qucVHcNbu2qnGkWWNwO0TRhY-7iH2\n",
            "To: /content/vietocr.zip\n",
            "100% 142M/142M [00:01<00:00, 101MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load publictest\n",
        "!gdown 1b2_B1HsssTCFBtLMG9xzndxmygM3TLCF\n",
        "!unzip -q public_test.zip"
      ],
      "metadata": {
        "id": "oM4ZwgYimIqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load config\n",
        "def load_config(yml_path):\n",
        "  with open(yml_path, \"r\") as stream:\n",
        "    try:\n",
        "      config = yaml.safe_load(stream)\n",
        "      return config\n",
        "    except yaml.YAMLError as exc:\n",
        "      print(exc)\n",
        "\n",
        "#Preprocessing\n",
        "def remove_background(image):\n",
        "  gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "  thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "  # Remove horizontal lines\n",
        "  horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40,1))\n",
        "  remove_horizontal = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel, iterations=1)\n",
        "  cnts = cv2.findContours(remove_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "  for c in cnts:\n",
        "    cv2.drawContours(thresh, [c], -1, (0,255,255), 5)\n",
        "  # Remove vertical lines\n",
        "  vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,40))\n",
        "  remove_vertical = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, vertical_kernel, iterations=2)\n",
        "  cnts = cv2.findContours(remove_vertical, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "  for c in cnts:\n",
        "    cv2.drawContours(thresh, [c], -1, (0,255,255), 15)\n",
        "  contours,hierarchy = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n",
        "  bounding_boxes = []\n",
        "  for cnt in contours:\n",
        "    if cv2.contourArea(cnt)>50:\n",
        "      [x,y,w,h] = cv2.boundingRect(cnt)\n",
        "      if (x, y) != (0, 0) and (y+h-y)/image.shape[0] > 0.18 and (x+w-x)/image.shape[1] > 0.01 and (x+w-x)/(y+h-y) > 0.15:\n",
        "        bounding_boxes.append([x,y,x+w,y+h])\n",
        "  if np.array(bounding_boxes).size != 0:\n",
        "    yymin = min(np.array(bounding_boxes)[:, 1])\n",
        "    xxmax = max(np.array(bounding_boxes)[:, 2])\n",
        "    yymax = max(np.array(bounding_boxes)[:, 3])\n",
        "    xxmax = xxmax + 40 if xxmax + 40 < image.shape[1] else xxmax\n",
        "    yymin = yymin - 5 if yymin - 5 > 0 else yymin\n",
        "    yymax = yymax + 5 if yymax + 5 < image.shape[0] else yymax\n",
        "    return image[yymin:yymax, 0:xxmax]\n",
        "  else:\n",
        "    return image\n",
        "\n",
        "#Inference\n",
        "def prediction(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  processed_img = remove_background(img)\n",
        "  img = Image.fromarray(processed_img)\n",
        "  s = detector.predict(img, return_prob = False)\n",
        "  return s\n",
        "\n",
        "def predict(image_folder, output_file_path):\n",
        "    prediction = pd.DataFrame(columns=['id', 'answer', 'elapsed_time'])\n",
        "    index = 0\n",
        "    for person_id in os.listdir(image_folder):\n",
        "        for image_id in os.listdir(os.path.join(image_folder, person_id)):\n",
        "            fp = os.path.join(image_folder, person_id, image_id)\n",
        "            image_id = os.path.join(person_id, image_id)\n",
        "            image = cv2.imread(fp)\n",
        "\n",
        "            # Start inference\n",
        "            start = time.time()\n",
        "            processed_img = remove_background(image) # preprocess\n",
        "            img = Image.fromarray(processed_img)\n",
        "            answer = s = detector.predict(img, return_prob = False) # infer\n",
        "            answer = answer.replace('Đp', 'Đg').replace('đp', 'đg') # post process\n",
        "            end = time.time()\n",
        "\n",
        "            prediction.loc[index] = [image_id, answer, end - start]\n",
        "            index += 1\n",
        "    # Write prediction\n",
        "    prediction.to_csv(output_file_path, index=False)"
      ],
      "metadata": {
        "id": "CPcmd84iwIAp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference without using quantization"
      ],
      "metadata": {
        "id": "YYstByH8l-0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Update config\n",
        "config = load_config('/content/base.yml')\n",
        "config['weights'] = '/content/weights/transformerocr.pth' #trained on our modified dataset\n",
        "\n",
        "#Load predictor\n",
        "detector = Predictor(config)"
      ],
      "metadata": {
        "id": "0PIBMfHwwCvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "predict(\"/content/public_test/images\", \"team_00_private_test_pred.csv\")\n",
        "pd.read_csv('/content/team_00_private_test_pred.csv')[['id', 'answer']].to_csv('private_test_pred.csv', index = False)"
      ],
      "metadata": {
        "id": "5kEfBgJll7iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_modelsize(config):\n",
        "  model, vocab = build_model(config)\n",
        "  weights = config['weights']\n",
        "  model.load_state_dict(torch.load(weights, map_location=torch.device(config['device'])))\n",
        "  param_size = 0\n",
        "  for param in model.parameters():\n",
        "      param_size += param.nelement() * param.element_size()\n",
        "  buffer_size = 0\n",
        "  for buffer in model.buffers():\n",
        "      buffer_size += buffer.nelement() * buffer.element_size()\n",
        "  size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "  print('model size: {:.3f}MB'.format(size_all_mb))\n",
        "\n",
        "check_modelsize(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ek03rXDwSzG",
        "outputId": "1408e450-2527-4c5c-d579-f56c00f143ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 144.666MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference using quantization"
      ],
      "metadata": {
        "id": "YaJcbsO6lFbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedCNN(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedCNN, self).__init__()\n",
        "        self.quant = torch.quantization.QuantStub() # QuantStub converts tensors from floating point to quantized. This will only be used for inputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub() # DeQuantStub converts tensors from quantized to floating point. This will only be used for outputs.\n",
        "        self.model_fp32 = model_fp32 # FP32 model\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x) #manually specify where tensors will be converted from floating point to quantized in the quantized model\n",
        "        x = self.model_fp32(x)\n",
        "        x = self.dequant(x) #manually specify where tensors will be converted from quantized to floating point in the quantized model\n",
        "        return x\n",
        "\n",
        "def build_quantized_model(config):\n",
        "  model, vocab = build_model(config)\n",
        "  model = model.eval()\n",
        "  for m in model.cnn.model.modules():\n",
        "      if type(m) == nn.Sequential:\n",
        "          for n, layer in enumerate(m):\n",
        "              if type(layer) == nn.Conv2d:\n",
        "                  torch.quantization.fuse_modules(m, [str(n), str(n + 1), str(n + 2)], inplace=True)\n",
        "  quantized_cnn = QuantizedCNN(model_fp32=model.cnn)\n",
        "  quantized_cnn.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "  quantized_cnn = torch.quantization.prepare_qat(quantized_cnn, inplace=True)\n",
        "  quantized_cnn = quantized_cnn.to(torch.device('cpu'))\n",
        "  model.cnn = torch.quantization.convert(quantized_cnn, inplace=True)\n",
        "  return model, vocab\n",
        "\n",
        "class Predictor():\n",
        "    def __init__(self, config, quantize = False):\n",
        "        device = config['device']\n",
        "        if quantize:\n",
        "            model, vocab = build_quantized_model(config)\n",
        "            model.load_state_dict(torch.load(config['quantize_weights']), strict=False)\n",
        "        else:\n",
        "            model, vocab = build_model(config)\n",
        "            model.load_state_dict(torch.load(config['weights'], map_location=torch.device(device)))\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.vocab = vocab\n",
        "        self.device = device\n",
        "    def predict(self, img, return_prob=False):\n",
        "        img = process_input(img, self.config['dataset']['image_height'],\n",
        "                self.config['dataset']['image_min_width'], self.config['dataset']['image_max_width'])\n",
        "        img = img.to(self.config['device'])\n",
        "        if self.config['predictor']['beamsearch']:\n",
        "            sent = translate_beam_search(img, self.model)\n",
        "            s = sent\n",
        "            prob = None\n",
        "        else:\n",
        "            s, prob = translate(img, self.model)\n",
        "            s = s[0].tolist()\n",
        "            prob = prob[0]\n",
        "        s = self.vocab.decode(s)\n",
        "        if return_prob:\n",
        "            return s, prob\n",
        "        else:\n",
        "            return s\n",
        "    def predict_batch(self, imgs, return_prob=False):\n",
        "        bucket = defaultdict(list)\n",
        "        bucket_idx = defaultdict(list)\n",
        "        bucket_pred = {}\n",
        "        sents, probs = [0]*len(imgs), [0]*len(imgs)\n",
        "        for i, img in enumerate(imgs):\n",
        "            img = process_input(img, self.config['dataset']['image_height'],\n",
        "                self.config['dataset']['image_min_width'], self.config['dataset']['image_max_width'])\n",
        "            bucket[img.shape[-1]].append(img)\n",
        "            bucket_idx[img.shape[-1]].append(i)\n",
        "        for k, batch in bucket.items():\n",
        "            batch = torch.cat(batch, 0).to(self.device)\n",
        "            s, prob = translate(batch, self.model)\n",
        "            prob = prob.tolist()\n",
        "            s = s.tolist()\n",
        "            s = self.vocab.batch_decode(s)\n",
        "            bucket_pred[k] = (s, prob)\n",
        "        for k in bucket_pred:\n",
        "            idx = bucket_idx[k]\n",
        "            sent, prob = bucket_pred[k]\n",
        "            for i, j in enumerate(idx):\n",
        "                sents[j] = sent[i]\n",
        "                probs[j] = prob[i]\n",
        "        if return_prob:\n",
        "            return sents, probs\n",
        "        else:\n",
        "            return sents"
      ],
      "metadata": {
        "id": "YkNW4yLxeVE8"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1VFpWftC5ECR3--GLWlwfsdcN3s-hxLbp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvVSaEakgBrs",
        "outputId": "ac6c56e0-5783-4df3-f7fc-a17158c6af04"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VFpWftC5ECR3--GLWlwfsdcN3s-hxLbp\n",
            "To: /content/quantize_transformerocr.pth\n",
            "100% 91.4M/91.4M [00:08<00:00, 11.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Update config\n",
        "config = load_config('./base.yml')\n",
        "config['device'] = 'cpu'\n",
        "config['cnn']['pretrained']=False\n",
        "config['quantize_weights'] = \"./quantize_transformerocr.pth\""
      ],
      "metadata": {
        "id": "UkedJHQ5j6wG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create detector\n",
        "detector = Predictor(config, quantize = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Yqvyn3kch1b",
        "outputId": "7f21f0a1-e2e5-489f-85ac-6e88ab2f1e44"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "predict(\"/content/public_test/images\", \"team_00_private_test_pred.csv\")\n",
        "pd.read_csv('/content/team_00_private_test_pred.csv')[['id', 'answer']].to_csv('private_test_pred.csv', index = False)"
      ],
      "metadata": {
        "id": "EYa0Iw2nwRHc"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_modelsize(config):\n",
        "  model, vocab = build_quantized_model(config)\n",
        "  model.load_state_dict(torch.load(config['quantize_weights']), strict=False)\n",
        "  param_size = 0\n",
        "  for param in model.parameters():\n",
        "      param_size += param.nelement() * param.element_size()\n",
        "  buffer_size = 0\n",
        "  for buffer in model.buffers():\n",
        "      buffer_size += buffer.nelement() * buffer.element_size()\n",
        "  size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "  print('model size: {:.3f}MB'.format(size_all_mb))\n",
        "\n",
        "check_modelsize(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgHidFl8jTjG",
        "outputId": "4bb2bdce-da2d-4d5f-9787-604705c59626"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 67.694MB\n"
          ]
        }
      ]
    }
  ]
}