{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U scikit-image\n",
        "!pip -q install einops==0.2.0\n",
        "!pip -q install gdown==4.4.0\n",
        "!pip -q install prefetch_generator==1.0.1\n",
        "!pip -q install imgaug==0.4.0\n",
        "!pip -q install lmdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlS3gsb67NSx",
        "outputId": "79838d56-9e58-4c79-c889-b8f7d2824f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for prefetch_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ifdb5Zg5MAV",
        "outputId": "c59f3ada-aebb-4583-d4f1-59a85eba1775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load model\n",
        "!gdown 1VU-qucVHcNbu2qnGkWWNwO0TRhY-7iH2\n",
        "!unzip -q vietocr.zip\n",
        "!unzip -q weights.zip\n",
        "\n",
        "#!cp '/content/drive/MyDrive/2023-Projects/Kalapa 2023/Vietnamese Handwritten Text Recognition/model/vietocr.zip' ./\n",
        "#!unzip -q vietocr.zip\n",
        "#!unzip -q weights.zip"
      ],
      "metadata": {
        "id": "tvGoI_mbzRUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vietocr import *"
      ],
      "metadata": {
        "id": "dS4e3ALvzRRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PIL\n",
        "import copy\n",
        "import yaml\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "from einops import rearrange\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam, SGD, AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CyclicLR, OneCycleLR"
      ],
      "metadata": {
        "id": "452h3Iof5y4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load predictor\n",
        "def load_config(yml_path):\n",
        "  with open(yml_path, \"r\") as stream:\n",
        "    try:\n",
        "      config = yaml.safe_load(stream)\n",
        "      return config\n",
        "    except yaml.YAMLError as exc:\n",
        "      print(exc)\n",
        "\n",
        "def save_models(model, file_name):\n",
        "    output_path = './weights/'\n",
        "    if not os.path.exists(output_path):\n",
        "        os.mkdir(output_path)\n",
        "    saved_path = os.path.join(output_path, file_name)\n",
        "    if os.path.exists(saved_path):\n",
        "        os.remove(saved_path)\n",
        "    print('Save files in: ', saved_path)\n",
        "    torch.save(model.state_dict(), saved_path)\n",
        "def save_torchscript_model(model, file_name):\n",
        "    output_path = './weights/'\n",
        "    if not os.path.exists(output_path):\n",
        "        os.mkdir(output_path)\n",
        "    model_filepath = os.path.join(output_path, file_name)\n",
        "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
        "    print('Save in: ', model_filepath)\n",
        "    return model_filepath\n",
        "def load_torchscript_model(model_filepath, device):\n",
        "    model = torch.jit.load(model_filepath, map_location=device)\n",
        "    return model\n",
        "\n",
        "class QuantizedCNN(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedCNN, self).__init__()\n",
        "        self.quant = torch.quantization.QuantStub() # QuantStub converts tensors from floating point to quantized. This will only be used for inputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub() # DeQuantStub converts tensors from quantized to floating point. This will only be used for outputs.\n",
        "        self.model_fp32 = model_fp32 # FP32 model\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x) #manually specify where tensors will be converted from floating point to quantized in the quantized model\n",
        "        x = self.model_fp32(x)\n",
        "        x = self.dequant(x) #manually specify where tensors will be converted from quantized to floating point in the quantized model\n",
        "        return x"
      ],
      "metadata": {
        "id": "foIXQJn5zV-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Update Trainer\n",
        "class Trainer():\n",
        "    def __init__(self, config, qmodel=None, pretrained=True, augmentor=ImgAugTransform()):\n",
        "        self.config = config\n",
        "        if qmodel is not None:\n",
        "            _, self.vocab = build_model(config)\n",
        "            self.model = qmodel\n",
        "        else:\n",
        "            self.model, self.vocab = build_model(config)\n",
        "        self.device = config['device']\n",
        "        self.num_iters = config['trainer']['iters']\n",
        "        self.beamsearch = config['predictor']['beamsearch']\n",
        "        self.data_root = config['dataset']['data_root']\n",
        "        self.train_annotation = config['dataset']['train_annotation']\n",
        "        self.valid_annotation = config['dataset']['valid_annotation']\n",
        "        self.dataset_name = config['dataset']['name']\n",
        "        self.batch_size = config['trainer']['batch_size']\n",
        "        self.print_every = config['trainer']['print_every']\n",
        "        self.valid_every = config['trainer']['valid_every']\n",
        "        self.image_aug = config['aug']['image_aug']\n",
        "        self.masked_language_model = config['aug']['masked_language_model']\n",
        "        self.checkpoint = config['trainer']['checkpoint']\n",
        "        self.export_weights = config['trainer']['export']\n",
        "        self.metrics = config['trainer']['metrics']\n",
        "        logger = config['trainer']['log']\n",
        "        if logger:\n",
        "            self.logger = Logger(logger)\n",
        "        if pretrained:\n",
        "            weight_file = download_weights(config['pretrain'], quiet=config['quiet'])\n",
        "            self.load_weights(weight_file)\n",
        "        self.iter = 0\n",
        "        self.optimizer = AdamW(self.model.parameters(), betas=(0.9, 0.98), eps=1e-09)\n",
        "        self.scheduler = OneCycleLR(self.optimizer, total_steps=self.num_iters, **config['optimizer'])\n",
        "        self.criterion = LabelSmoothingLoss(len(self.vocab), padding_idx=self.vocab.pad, smoothing=0.1)\n",
        "        transforms = None\n",
        "        if self.image_aug:\n",
        "            transforms =  augmentor\n",
        "        self.train_gen = self.data_gen('train_{}'.format(self.dataset_name),\n",
        "                self.data_root, self.train_annotation, self.masked_language_model, transform=transforms)\n",
        "        if self.valid_annotation:\n",
        "            self.valid_gen = self.data_gen('valid_{}'.format(self.dataset_name),\n",
        "                    self.data_root, self.valid_annotation, masked_language_model=False)\n",
        "        self.train_losses = []\n",
        "    def train(self):\n",
        "        total_loss = 0\n",
        "        total_loader_time = 0\n",
        "        total_gpu_time = 0\n",
        "        best_acc = 0\n",
        "        data_iter = iter(self.train_gen)\n",
        "        for i in range(self.num_iters):\n",
        "            self.iter += 1\n",
        "            start = time.time()\n",
        "            try:\n",
        "                batch = next(data_iter)\n",
        "            except StopIteration:\n",
        "                data_iter = iter(self.train_gen)\n",
        "                batch = next(data_iter)\n",
        "            total_loader_time += time.time() - start\n",
        "            start = time.time()\n",
        "            loss = self.step(batch)\n",
        "            total_gpu_time += time.time() - start\n",
        "            total_loss += loss\n",
        "            self.train_losses.append((self.iter, loss))\n",
        "            if self.iter % self.print_every == 0:\n",
        "                info = 'iter: {:06d} - train loss: {:.3f} - lr: {:.2e} - load time: {:.2f} - gpu time: {:.2f}'.format(self.iter,\n",
        "                        total_loss/self.print_every, self.optimizer.param_groups[0]['lr'],\n",
        "                        total_loader_time, total_gpu_time)\n",
        "                total_loss = 0\n",
        "                total_loader_time = 0\n",
        "                total_gpu_time = 0\n",
        "                print(info)\n",
        "                self.logger.log(info)\n",
        "            if self.valid_annotation and self.iter % self.valid_every == 0:\n",
        "                val_loss = self.validate()\n",
        "                acc_full_seq, acc_per_char = self.precision(self.metrics)\n",
        "                info = 'iter: {:06d} - valid loss: {:.3f} - acc full seq: {:.4f} - acc per char: {:.4f}'.format(self.iter, val_loss, acc_full_seq, acc_per_char)\n",
        "                print(info)\n",
        "                self.logger.log(info)\n",
        "                if acc_full_seq > best_acc:\n",
        "                    self.save_weights(self.export_weights)\n",
        "                    best_acc = acc_full_seq\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = []\n",
        "        with torch.no_grad():\n",
        "            for step, batch in enumerate(self.valid_gen):\n",
        "                batch = self.batch_to_device(batch)\n",
        "                img, tgt_input, tgt_output, tgt_padding_mask = batch['img'], batch['tgt_input'], batch['tgt_output'], batch['tgt_padding_mask']\n",
        "                outputs = self.model(img, tgt_input, tgt_padding_mask)\n",
        "                outputs = outputs.flatten(0,1)\n",
        "                tgt_output = tgt_output.flatten()\n",
        "                loss = self.criterion(outputs, tgt_output)\n",
        "                total_loss.append(loss.item())\n",
        "                del outputs\n",
        "                del loss\n",
        "        total_loss = np.mean(total_loss)\n",
        "        self.model.train()\n",
        "        return total_loss\n",
        "    def predict(self, sample=None):\n",
        "        pred_sents = []\n",
        "        actual_sents = []\n",
        "        img_files = []\n",
        "        for batch in  self.valid_gen:\n",
        "            batch = self.batch_to_device(batch)\n",
        "            if self.beamsearch:\n",
        "                translated_sentence = batch_translate_beam_search(batch['img'], self.model)\n",
        "                prob = None\n",
        "            else:\n",
        "                translated_sentence, prob = translate(batch['img'], self.model)\n",
        "            pred_sent = self.vocab.batch_decode(translated_sentence.tolist())\n",
        "            actual_sent = self.vocab.batch_decode(batch['tgt_output'].tolist())\n",
        "            img_files.extend(batch['filenames'])\n",
        "            pred_sents.extend(pred_sent)\n",
        "            actual_sents.extend(actual_sent)\n",
        "            if sample != None and len(pred_sents) > sample:\n",
        "                break\n",
        "        return pred_sents, actual_sents, img_files, prob\n",
        "    def precision(self, sample=None):\n",
        "        pred_sents, actual_sents, _, _ = self.predict(sample=sample)\n",
        "        acc_full_seq = compute_accuracy(actual_sents, pred_sents, mode='full_sequence')\n",
        "        acc_per_char = compute_accuracy(actual_sents, pred_sents, mode='per_char')\n",
        "        return acc_full_seq, acc_per_char\n",
        "    def visualize_prediction(self, sample=16, errorcase=False, fontname='serif', fontsize=16):\n",
        "        pred_sents, actual_sents, img_files, probs = self.predict(sample)\n",
        "        if errorcase:\n",
        "            wrongs = []\n",
        "            for i in range(len(img_files)):\n",
        "                if pred_sents[i]!= actual_sents[i]:\n",
        "                    wrongs.append(i)\n",
        "            pred_sents = [pred_sents[i] for i in wrongs]\n",
        "            actual_sents = [actual_sents[i] for i in wrongs]\n",
        "            img_files = [img_files[i] for i in wrongs]\n",
        "            probs = [probs[i] for i in wrongs]\n",
        "        img_files = img_files[:sample]\n",
        "        fontdict = {'family':fontname, 'size':fontsize}\n",
        "        for vis_idx in range(0, len(img_files)):\n",
        "            img_path = img_files[vis_idx]\n",
        "            pred_sent = pred_sents[vis_idx]\n",
        "            actual_sent = actual_sents[vis_idx]\n",
        "            prob = probs[vis_idx]\n",
        "            img = Image.open(open(img_path, 'rb'))\n",
        "            plt.figure()\n",
        "            plt.imshow(img)\n",
        "            plt.title('prob: {:.3f} - pred: {} - actual: {}'.format(prob, pred_sent, actual_sent), loc='left', fontdict=fontdict)\n",
        "            plt.axis('off')\n",
        "        plt.show()\n",
        "    def visualize_dataset(self, sample=16, fontname='serif'):\n",
        "        n = 0\n",
        "        for batch in self.train_gen:\n",
        "            for i in range(self.batch_size):\n",
        "                img = batch['img'][i].numpy().transpose(1,2,0)\n",
        "                sent = self.vocab.decode(batch['tgt_input'].T[i].tolist())\n",
        "                plt.figure()\n",
        "                plt.title('sent: {}'.format(sent), loc='center', fontname=fontname)\n",
        "                plt.imshow(img)\n",
        "                plt.axis('off')\n",
        "                n += 1\n",
        "                if n >= sample:\n",
        "                    plt.show()\n",
        "                    return\n",
        "    def load_checkpoint(self, filename):\n",
        "        checkpoint = torch.load(filename)\n",
        "        optim = ScheduledOptim(\n",
        "\t       Adam(self.model.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
        "            \tself.config['transformer']['d_model'], **self.config['optimizer'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.model.load_state_dict(checkpoint['state_dict'])\n",
        "        self.iter = checkpoint['iter']\n",
        "        self.train_losses = checkpoint['train_losses']\n",
        "    def save_checkpoint(self, filename):\n",
        "        state = {'iter':self.iter, 'state_dict': self.model.state_dict(),\n",
        "                'optimizer': self.optimizer.state_dict(), 'train_losses': self.train_losses}\n",
        "        path, _ = os.path.split(filename)\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        torch.save(state, filename)\n",
        "    def load_weights(self, filename):\n",
        "        state_dict = torch.load(filename, map_location=torch.device(self.device))\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if name not in state_dict:\n",
        "                print('{} not found'.format(name))\n",
        "            elif state_dict[name].shape != param.shape:\n",
        "                print('{} missmatching shape, required {} but found {}'.format(name, param.shape, state_dict[name].shape))\n",
        "                del state_dict[name]\n",
        "        self.model.load_state_dict(state_dict, strict=False)\n",
        "    def save_weights(self, filename):\n",
        "        path, _ = os.path.split(filename)\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        qmodel = copy.deepcopy(self.model)\n",
        "        qmodel.to(torch.device('cpu'))\n",
        "        qmodel.cnn = torch.quantization.convert(qmodel.cnn.eval(), inplace=True)\n",
        "        qmodel.eval()\n",
        "        torch.save(qmodel.state_dict(), filename)\n",
        "    def batch_to_device(self, batch):\n",
        "        img = batch['img'].to(self.device, non_blocking=True)\n",
        "        tgt_input = batch['tgt_input'].to(self.device, non_blocking=True)\n",
        "        tgt_output = batch['tgt_output'].to(self.device, non_blocking=True)\n",
        "        tgt_padding_mask = batch['tgt_padding_mask'].to(self.device, non_blocking=True)\n",
        "        batch = {'img': img, 'tgt_input':tgt_input, 'tgt_output':tgt_output, 'tgt_padding_mask':tgt_padding_mask, 'filenames': batch['filenames']}\n",
        "        return batch\n",
        "    def data_gen(self, lmdb_path, data_root, annotation, masked_language_model=True, transform=None):\n",
        "        dataset = OCRDataset(lmdb_path=lmdb_path,\n",
        "                root_dir=data_root, annotation_path=annotation,\n",
        "                vocab=self.vocab, transform=transform,\n",
        "                image_height=self.config['dataset']['image_height'],\n",
        "                image_min_width=self.config['dataset']['image_min_width'],\n",
        "                image_max_width=self.config['dataset']['image_max_width'])\n",
        "        sampler = ClusterRandomSampler(dataset, self.batch_size, True)\n",
        "        collate_fn = Collator(masked_language_model)\n",
        "        gen = DataLoader(dataset, batch_size=self.batch_size, sampler=sampler,\n",
        "                         collate_fn = collate_fn, shuffle=False, drop_last=False, **self.config['dataloader'])\n",
        "        return gen\n",
        "    def data_gen_v1(self, lmdb_path, data_root, annotation):\n",
        "        data_gen = DataGen(data_root, annotation, self.vocab, 'cpu',\n",
        "                image_height = self.config['dataset']['image_height'],\n",
        "                image_min_width = self.config['dataset']['image_min_width'],\n",
        "                image_max_width = self.config['dataset']['image_max_width'])\n",
        "        return data_gen\n",
        "    def step(self, batch):\n",
        "        self.model.train()\n",
        "        batch = self.batch_to_device(batch)\n",
        "        img, tgt_input, tgt_output, tgt_padding_mask = batch['img'], batch['tgt_input'], batch['tgt_output'], batch['tgt_padding_mask']\n",
        "        outputs = self.model(img, tgt_input, tgt_key_padding_mask=tgt_padding_mask)\n",
        "        outputs = outputs.view(-1, outputs.size(2))#flatten(0, 1)\n",
        "        tgt_output = tgt_output.view(-1)#flatten()\n",
        "        loss = self.criterion(outputs, tgt_output)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1)\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "        loss_item = loss.item()\n",
        "        return loss_item"
      ],
      "metadata": {
        "id": "gsz2g_H84SCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load dataset\n",
        "!gdown 1LaWUP_jCaoJJeEOOYHJEUvtTaGByT3c7\n",
        "!unzip -q quantization_data.zip\n",
        "#!cp '/content/drive/MyDrive/2023-Projects/Kalapa 2023/Vietnamese Handwritten Text Recognition/dataset/quantization_data.zip' ./\n",
        "#!unzip -q quantization_data.zip"
      ],
      "metadata": {
        "id": "uUBdt8D1VaOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Update config\n",
        "config = load_config('/content/base.yml')\n",
        "dataset_params = {'name':'full_dataset',\n",
        "                  'data_root':'./quantization_data',\n",
        "                  'train_annotation':'train_line_annotation.txt',\n",
        "                  'valid_annotation':'test_line_annotation.txt'}\n",
        "params = {'batch_size': 1, 'print_every': 200, 'valid_every': 400,'iters': 5400,\n",
        "          'checkpoint':'./quantize_checkpoint/quantize_transformerocr.pth',\n",
        "          'export':'./quantize_weights/quantize_transformerocr.pth',\n",
        "          'log': './train.log', 'metrics': None}\n",
        "\n",
        "config['trainer'].update(params)\n",
        "config['dataset'].update(dataset_params)\n",
        "config['weights'] = \"./weights/transformerocr.pth\"\n",
        "config['cnn']['pretrained']=False\n",
        "device = config['device']\n",
        "config"
      ],
      "metadata": {
        "id": "EhlXy3KfsYRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de45511f-e295-4caa-bf9b-3341aa5189fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab': 'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ ',\n",
              " 'device': 'cpu',\n",
              " 'seq_modeling': 'transformer',\n",
              " 'transformer': {'d_model': 256,\n",
              "  'nhead': 8,\n",
              "  'num_encoder_layers': 6,\n",
              "  'num_decoder_layers': 6,\n",
              "  'dim_feedforward': 2048,\n",
              "  'max_seq_length': 1024,\n",
              "  'pos_dropout': 0.1,\n",
              "  'trans_dropout': 0.1},\n",
              " 'optimizer': {'max_lr': 0.0003, 'pct_start': 0.1},\n",
              " 'trainer': {'batch_size': 1,\n",
              "  'print_every': 200,\n",
              "  'valid_every': 400,\n",
              "  'iters': 5400,\n",
              "  'export': './quantize_weights/quantize_transformerocr.pth',\n",
              "  'checkpoint': './quantize_checkpoint/quantize_transformerocr.pth',\n",
              "  'log': './train.log',\n",
              "  'metrics': None},\n",
              " 'dataset': {'name': 'full_dataset',\n",
              "  'data_root': './quantization_data',\n",
              "  'train_annotation': 'train_line_annotation.txt',\n",
              "  'valid_annotation': 'test_line_annotation.txt',\n",
              "  'image_height': 32,\n",
              "  'image_min_width': 32,\n",
              "  'image_max_width': 512},\n",
              " 'dataloader': {'num_workers': 0, 'pin_memory': True},\n",
              " 'aug': {'image_aug': True, 'masked_language_model': True},\n",
              " 'predictor': {'beamsearch': False},\n",
              " 'quiet': False,\n",
              " 'pretrain': 'https://vocr.vn/data/vietocr/vgg_transformer.pth',\n",
              " 'weights': './weights/transformerocr.pth',\n",
              " 'backbone': 'vgg19_bn',\n",
              " 'cnn': {'pretrained': False,\n",
              "  'ss': [[2, 2], [2, 2], [2, 1], [2, 1], [1, 1]],\n",
              "  'ks': [[2, 2], [2, 2], [2, 1], [2, 1], [1, 1]],\n",
              "  'hidden': 256}}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fuse layer\n",
        "model, vocab = build_model(config)\n",
        "weights = config['weights']\n",
        "model.load_state_dict(torch.load(weights, map_location=torch.device(device)))\n",
        "model = model.eval()\n",
        "for m in model.cnn.model.modules():\n",
        "    if type(m) == nn.Sequential:\n",
        "        for n, layer in enumerate(m):\n",
        "            if type(layer) == nn.Conv2d:\n",
        "                torch.quantization.fuse_modules(m, [str(n), str(n + 1), str(n + 2)], inplace=True)\n",
        "\n",
        "# Prepare the model for quantization aware training.\n",
        "quantized_cnn = QuantizedCNN(model_fp32=model.cnn)\n",
        "quantized_cnn.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "# Print quantization configurations\n",
        "print(quantized_cnn.qconfig)\n",
        "# the prepare() is used in post training quantization to prepares your model for the calibration step quantized_cnn = torch.quantization.prepare_qat(quantized_cnn, inplace=True)\n",
        "quantized_cnn = torch.quantization.prepare_qat(quantized_cnn.train(), inplace=True)\n",
        "model.cnn = quantized_cnn\n",
        "\n",
        "model.train()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "U30e7Q7Osjl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Re-training\n",
        "trainer = Trainer(config=config, qmodel=model, pretrained=False)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "bgJ8ScpGsmFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a69c378-3d5e-4929-f71e-766e6d8c3604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Create train_full_dataset: 100%|█████████████████████████████████| 922/922 [00:05<00:00, 157.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 921 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_full_dataset build cluster: 100%|████████████████████████| 921/921 [00:00<00:00, 69443.87it/s]\n",
            "Create valid_full_dataset: 100%|█████████████████████████████████| 517/517 [00:03<00:00, 140.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 516 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "valid_full_dataset build cluster: 100%|████████████████████████| 516/516 [00:00<00:00, 41897.57it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 000200 - train loss: 0.997 - lr: 9.93e-05 - load time: 7.66 - gpu time: 415.41\n",
            "iter: 000400 - train loss: 1.148 - lr: 2.55e-04 - load time: 5.88 - gpu time: 420.43\n",
            "iter: 000400 - valid loss: 1.123 - acc full seq: 0.1531 - acc per char: 0.6499\n",
            "iter: 000600 - train loss: 1.317 - lr: 3.00e-04 - load time: 6.38 - gpu time: 402.53\n",
            "iter: 000800 - train loss: 1.301 - lr: 2.98e-04 - load time: 7.89 - gpu time: 404.06\n",
            "iter: 000800 - valid loss: 1.226 - acc full seq: 0.0717 - acc per char: 0.5082\n",
            "iter: 001000 - train loss: 1.336 - lr: 2.93e-04 - load time: 7.32 - gpu time: 406.37\n",
            "iter: 001200 - train loss: 1.297 - lr: 2.87e-04 - load time: 6.54 - gpu time: 407.34\n",
            "iter: 001200 - valid loss: 1.102 - acc full seq: 0.1860 - acc per char: 0.6636\n",
            "iter: 001400 - train loss: 1.281 - lr: 2.77e-04 - load time: 7.91 - gpu time: 409.95\n",
            "iter: 001600 - train loss: 1.283 - lr: 2.66e-04 - load time: 7.17 - gpu time: 410.15\n",
            "iter: 001600 - valid loss: 1.140 - acc full seq: 0.1453 - acc per char: 0.5773\n",
            "iter: 001800 - train loss: 1.299 - lr: 2.53e-04 - load time: 8.08 - gpu time: 416.48\n",
            "iter: 002000 - train loss: 1.218 - lr: 2.38e-04 - load time: 6.98 - gpu time: 428.75\n",
            "iter: 002000 - valid loss: 1.107 - acc full seq: 0.1919 - acc per char: 0.5806\n",
            "iter: 002200 - train loss: 1.200 - lr: 2.22e-04 - load time: 8.34 - gpu time: 415.77\n",
            "iter: 002400 - train loss: 1.115 - lr: 2.04e-04 - load time: 8.08 - gpu time: 420.36\n",
            "iter: 002400 - valid loss: 1.027 - acc full seq: 0.3062 - acc per char: 0.7202\n",
            "iter: 002600 - train loss: 1.150 - lr: 1.85e-04 - load time: 6.42 - gpu time: 429.31\n",
            "iter: 002800 - train loss: 1.091 - lr: 1.66e-04 - load time: 8.65 - gpu time: 427.75\n",
            "iter: 002800 - valid loss: 0.986 - acc full seq: 0.3798 - acc per char: 0.7783\n",
            "iter: 003000 - train loss: 1.068 - lr: 1.47e-04 - load time: 7.57 - gpu time: 426.35\n",
            "iter: 003200 - train loss: 1.051 - lr: 1.28e-04 - load time: 8.70 - gpu time: 431.00\n",
            "iter: 003200 - valid loss: 0.970 - acc full seq: 0.4419 - acc per char: 0.8094\n",
            "iter: 003400 - train loss: 1.024 - lr: 1.09e-04 - load time: 6.70 - gpu time: 433.97\n",
            "iter: 003600 - train loss: 0.993 - lr: 9.05e-05 - load time: 8.99 - gpu time: 432.47\n",
            "iter: 003600 - valid loss: 0.963 - acc full seq: 0.4554 - acc per char: 0.8428\n",
            "iter: 003800 - train loss: 0.991 - lr: 7.32e-05 - load time: 7.64 - gpu time: 440.28\n",
            "iter: 004000 - train loss: 0.958 - lr: 5.73e-05 - load time: 4.39 - gpu time: 444.29\n",
            "iter: 004000 - valid loss: 0.948 - acc full seq: 0.5078 - acc per char: 0.8490\n",
            "iter: 004200 - train loss: 0.962 - lr: 4.28e-05 - load time: 7.10 - gpu time: 449.89\n",
            "iter: 004400 - train loss: 0.948 - lr: 3.02e-05 - load time: 7.84 - gpu time: 458.95\n",
            "iter: 004400 - valid loss: 0.934 - acc full seq: 0.5717 - acc per char: 0.8818\n",
            "iter: 004600 - train loss: 0.955 - lr: 1.96e-05 - load time: 5.22 - gpu time: 451.11\n",
            "iter: 004800 - train loss: 0.941 - lr: 1.11e-05 - load time: 6.17 - gpu time: 455.69\n",
            "iter: 004800 - valid loss: 0.934 - acc full seq: 0.5756 - acc per char: 0.8732\n",
            "iter: 005000 - train loss: 0.942 - lr: 4.96e-06 - load time: 7.29 - gpu time: 457.05\n",
            "iter: 005200 - train loss: 0.948 - lr: 1.24e-06 - load time: 5.75 - gpu time: 462.71\n",
            "iter: 005200 - valid loss: 0.930 - acc full seq: 0.5833 - acc per char: 0.8786\n",
            "iter: 005400 - train loss: 0.920 - lr: 1.23e-09 - load time: 6.59 - gpu time: 459.55\n"
          ]
        }
      ]
    }
  ]
}